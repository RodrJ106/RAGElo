# RAGeval

<p align="center">
    <em>For when you are too lazy to annotate everything manually</em>
</p>

---
**Documentation**: soon

**Source Code**: [github.com/zetaalphavector/auto-eval](github.com/zetaalphavector/auto-eval)

---

RAGeval is a toolkit for evaluating answers generated by LLM-based agents using Retrieval Augmented Generation (RAG), based on the Elo ranking system.

The main features are:

- **Easy**: A simple and pretty CLI for comparing answers from multiple agents and annotating them.
- **Modular**: Easily change Evaluators, Agents and the ranking system itself (TODO)
- **Agnostic**: Evaluate answers generated with any agent, chain, model or LLM.

What RAGeval is **not**:

- A RAG system itself. It does not generate answers, it only evaluates them.
- A substitute for human evaluation. It is meant to be used as a tool to help with the annotation process, not to replace it.

## Requirements

Python 3.10+

RAGeval is built with [Typer](https://github.com/tiangolo/typer) for pretty CLIs.

## Installation

```bash
pip install rageval
```

## Naming

RAGeval deals with the following entities:

- **Agent**: A RAG system capable of generating answers for a query based on a list of documents retrieved by a search system.
- **Query**: A query or question answered by the *Agent*.
- **Document**: A passage or document that was used by the RAG system to generate an answer for the *query*.
- **Answer**: The answer generated by an *agent* for the *query* based on a set of *documents*. It should cite the *documents* used to generate it using square brackets (`[]`).
- **Evaluator**: The LLM to be used when evaluating the *answers* generated by the *agent*. It can be also used to annotate individual documents for relevance. It contains a *prompt* to be used when evaluating the *answers*.

## Example

For evaluating an answer, the first step is to annotate the documents used to generate it. This can be done using the `annotate-documents` command.

In this example, we are annotating the relevance for the documents retrieved for the queries in the `queries.csv` file. The documents are in the `documents.csv` file. We are using the `reasoner` evaluator, that only outputs the reasoning if the document is relevant or not. The annotations will be saved in the `annotations.csv` file.

```bash
$ python -m auto_eval annotate-documents queries.csv documents.csv reasoner outputs.csv 
```

## TODO

- [ ] Install as standalone CLI
